{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana collector project - DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a slightly modified Unity environment, to train and evaluate how our deep reinforcement learning algorithm is capable to learn by itself with the only help of rewards.\n",
    "\n",
    "The objective of the agent will be to wander around an square world to collect as much as possible yellow bananas avoiding the blue ones. \n",
    "\n",
    "A **reward of +1 is provided for collecting a yellow banana**, and a **reward of -1 is provided for collecting a blue banana**.\n",
    "\n",
    "The **state space has 37 dimensions** and contains the agent's velocity, along with ray-based perception of objects around the agent's forward direction. Given this information, the agent has to learn how to best select actions. **Four discrete actions** are available, corresponding to:\n",
    "\n",
    "* 0 - move forward.\n",
    "* 1 - move backward.\n",
    "* 2 - turn left.\n",
    "* 3 - turn right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/navigation_bananas.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal** to solve the environment is that the agent must get an **average score of +13 over 100 consecutive episodes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Deep Q-Network algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-learning algorithm, we saw at the beginning of the course, is a simple yet powerful algorithm, to create a cheat sheet for our agent. But in more complex and sophisticated environments it become computationally inefficient.<br>\n",
    "<br>\n",
    "So, instead of directly compute Q-values through iteration to find the optimal Q-function, we are going to approximate the optimal Q-function using Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, there are two main challenges in DRL compared to Deep Learning:\n",
    "* The target is continuously changing with each iteration. To solve this -> **TARGET NETWORK**. We create a separate duplicate of the network but with frozen parameters for C iterations. This will lead to more stable training because, for some fixe time steps, it keeps the target function fixed.<br><br>\n",
    "* Another possible issue we want to remove is the correlation between consecutive experience tuples -> **EXPERIENCE REPLAY**. With Experience Replay, we store the agent's expiriencies in a data set called Replay Memory. During training, we will randomly sample from this Replay Memory.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the pseudocode for the DQN algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/DQN algorithm.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Screenshot made from the Deep Reinforcement Learning Nanodegree from Udacity*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this steps in code, I have adapted the exercise \"Lunar Lander\" we previously did in the course. So, I finally have three files: DQN_Banana_Navigation.ipynb, dqn_agent.py and model.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **dqn_agent.py**: The Agent class is implemented with some important methods:\n",
    "    - Constructor where we initialize the instance variables state_size, action_size and seed. Also, we instantiate the local and target network, the Adam optimizer and the replay memory variable.\n",
    "    - step(self, state, action, reward, next_state, done): We use this method to save experiences in the replay memory. We will use this learnt experiences every 4 episodes.\n",
    "    - act(self, state, eps=0.): In charge to return an action for each state following an epsilon-greedy strategy.\n",
    "    - learn(self, experiences, gamma): We use a batch to optimize and update the parameters of the target network\n",
    "    - We also implement the class ReplayBuffer, that will allow us to add() experience tuples (state, action, reward, next_state, done) to the memory and sample() them for the learning and updating of the target network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **model.py**: I used the torch.nn package to build a NN in Pytorch. This package contains the class Module which is the base class for all NN modules. Our DQN is defined as a class that extends nn.Module. Our Network is made of two fully connected hidden layers and an output layer.<br>\n",
    "We also define a function called forward() that will implement a forward pass to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the model architecture chosen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    37 input notes -> ReLu(Fully connected layer (1024 nodes)) -> ReLu(Fully connected layer(1024 nodes)) -> 4 output nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **DQN_Banana_Navigation.ipynb**: In the Jupyter notebook, we make use of the previous functions in order to train and check the behaviour of the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size #original: int(1e5)\n",
    "BATCH_SIZE = 64         # minibatch size #original 64\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate  # original = 5e-4      \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "n_episodes = 2000       # max. number of episodes to complete the goal\n",
    "max_t = 1000            # max. number of time steps by episode\n",
    "eps_start = 1.0         # epsilon value at the beginning of each episode\n",
    "eps_end = 0.01          # min. epsilon value\n",
    "eps_decay = 0.95        # epsilon decay rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/results.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/results_plot.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that **we achieved the project requirements**, as the agent need **less than 500 episodes to receive an average reward greater than 13**, over 100 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm implemented to solve this problem follows the idea presented in the Deepmind's paper: [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf), where they developed a DQL algorithm that learned to play many Atari video games better than humans.<br><br>\n",
    "\n",
    "Nowadays, there are some upgrades to that original algorithm that we can use to improve our results. The three more important are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Double Deep-Q Networks**](https://arxiv.org/abs/1509.06461): *The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Dueling Deep-Q Networks**](https://arxiv.org/abs/1511.06581): *In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Prioritized experience replay**](https://arxiv.org/abs/1511.05952): *Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But these aren't the only improvements we can do to the DQN algorithm. There are some other important additions as: Learning from multi-step bootstrap targets, Distributional DQN, Noisy DQN..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible improvement we could add to this project is to train the agent directly from pixels instead of the environment's internal state.\n",
    "To do so, we would need to modify our model to add a Convolutional Neural Network in order to be able to process the raw pixel values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
